{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de optimización no lineal sin restricciones\n",
    "----------------\n",
    "\n",
    "## Autores\n",
    "* Quesada, Francisco\n",
    "* Serpe, Octavio\n",
    "* Arca, Gonzalo\n",
    "\n",
    "-----------------\n",
    "\n",
    "## Librerías utilizadas\n",
    "* SciPy\n",
    "* NumPy\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import time\n",
    "from autograd.wrap_util import wraps\n",
    "from autograd.misc import flatten\n",
    "import numdifftools as nd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REAL = -10\n",
    "MAX_REAL = 10\n",
    "\n",
    "NUMBER_OF_PARAMETERS = 11\n",
    "\n",
    "DATASET_INPUT = [(4.4793, -4.0765, -4.0765), (-4.1793, -\n",
    "                                              4.9218, 1.7664), (-3.9429, -0.7689, 4.8830)]\n",
    "\n",
    "DATASET_OUTPUT = [0, 1, 1]\n",
    "\n",
    "error_GD = -1;\n",
    "error_CG = -1;\n",
    "error_Adam = -1;\n",
    "\n",
    "x0 = np.random.uniform(low=MIN_REAL, high=MAX_REAL, size=NUMBER_OF_PARAMETERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W 3 element vector\n",
    "# w 2x3 matrix as 6 element vector\n",
    "# w_0 2 element vector\n",
    "# reactive_values 3 element vector\n",
    "def f(W, w, w_0, reactive_values):\n",
    "    inner_g_input = 0\n",
    "    outer_g_input = 0\n",
    "\n",
    "    for j in range(2):\n",
    "        for k in range(3):\n",
    "            inner_g_input += w[k + (j * 3)] * reactive_values[k] - w_0[j]\n",
    "        outer_g_input += W[j] * g(inner_g_input)\n",
    "    return g(outer_g_input - W[0])\n",
    "\n",
    "\n",
    "# dataset_input 3 element vector\n",
    "# dataset_output 3 element vector\n",
    "def error(x, dataset_input, dataset_output):\n",
    "    #dataset_input, dataset_output = dataset_tuple\n",
    "    error = 0\n",
    "\n",
    "    W = x[0:3]\n",
    "    w = x[3:9]\n",
    "    w_0 = x[9:11]\n",
    "\n",
    "    for i in range(len(dataset_input)):\n",
    "        error += math.pow(dataset_output[i] -\n",
    "                          f(W, w, w_0, dataset_input[i]), 2)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "def g(x):\n",
    "    # If over the limit, return the limit value\n",
    "    try:\n",
    "        exp_value = math.exp(x)\n",
    "    except OverflowError:\n",
    "        return 1\n",
    "\n",
    "    return exp_value / (1 + exp_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(x, time, error,iterations):\n",
    "  print(f'Argumentos óptimos:\\nW = \\t{x[0:3]}\\nw = \\t{x[3:6]}\\n\\t{x[6:9]}\\nw_0 = \\t{x[9:11]}\\n')\n",
    "  print(f'error: {error}')\n",
    "  print(\"Tiempo de ejecucion:\", time, \"segundos\")\n",
    "  print(\"Iteraciones:\", iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 1: Gradiente descendiente (método quasi-Newton BFGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000004\n",
      "         Iterations: 54\n",
      "         Function evaluations: 960\n",
      "         Gradient evaluations: 80\n",
      "Argumentos óptimos:\n",
      "W = \t[ 6.25645037 12.48471233 -8.86335845]\n",
      "w = \t[5.32071013 0.24126788 8.69307608]\n",
      "\t[-9.07505548 -6.30857524  7.0068466 ]\n",
      "w_0 = \t[-2.44128025 -2.96060102]\n",
      "\n",
      "error: 3.977474501761863e-06\n",
      "Tiempo de ejecucion: 0.02959451400238322 segundos\n",
      "Iteraciones: 54\n"
     ]
    }
   ],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html\n",
    "\n",
    "\n",
    "def gradient_descent(x0,print=True):\n",
    "  start_time = time.perf_counter()\n",
    "  optimize_result_GD = minimize(error, x0, method='BFGS', args=(DATASET_INPUT, DATASET_OUTPUT), tol=1e-5, options={\"disp\":print})\n",
    "  end_time = time.perf_counter()\n",
    "  error_GD = error(optimize_result_GD.x, DATASET_INPUT, DATASET_OUTPUT)\n",
    "  return optimize_result_GD.x,end_time-start_time,error_GD,optimize_result_GD.nit\n",
    "\n",
    "optimal_x_GD,time_GD,error_GD,nit = gradient_descent(x0)\n",
    "print_output(optimal_x_GD,time_GD,error_GD,nit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 2: Gradientes conjugados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000004\n",
      "         Iterations: 5\n",
      "         Function evaluations: 348\n",
      "         Gradient evaluations: 29\n",
      "Argumentos óptimos:\n",
      "W = \t[ 6.2215578  14.13941911 -8.86335845]\n",
      "w = \t[ 0.39650196  3.16421923 12.81062374]\n",
      "\t[-6.27393286 -8.85780686  4.45761493]\n",
      "w_0 = \t[-0.17909426 -4.83664734]\n",
      "\n",
      "error: 3.929164750793699e-06\n",
      "Tiempo de ejecucion: 0.014028710997081362 segundos\n",
      "Iteraciones: 5\n"
     ]
    }
   ],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html\n",
    "\n",
    "def conjugate_gradient(x0,print=True):\n",
    "  start_time = time.perf_counter()\n",
    "  optimize_result_CG = minimize(error, x0, method='CG', args=(DATASET_INPUT, DATASET_OUTPUT), tol=1e-5, options={\"disp\":print})\n",
    "  end_time = time.perf_counter()\n",
    "  error_CG = error(optimize_result_CG.x, DATASET_INPUT, DATASET_OUTPUT)\n",
    "  return optimize_result_CG.x,end_time-start_time,error_CG,optimize_result_CG.nit\n",
    "\n",
    "optimal_x_CG,time_CG,error_CG,nit = conjugate_gradient(x0)\n",
    "\n",
    "print_output(optimal_x_CG,time_CG,error_CG,nit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 3: ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion Método ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(f,grad, x,callback=None, num_iters=10000,\n",
    "         step_size=0.001, b1=0.9, b2=0.999, eps=10**-8,tol=1e-5):\n",
    "    \"\"\"Adam as described in http://arxiv.org/pdf/1412.6980.pdf.\n",
    "    It's basically RMSprop with momentum and some correction terms.\"\"\"\n",
    "    m = np.zeros(len(x))\n",
    "    v = np.zeros(len(x))\n",
    "    for i in range(num_iters):\n",
    "        error = f(x,DATASET_INPUT,DATASET_OUTPUT)\n",
    "        if error < tol:\n",
    "            break;\n",
    "        g = grad(x, i)\n",
    "        if callback: callback(x, i, g)\n",
    "        m = (1 - b1) * g      + b1 * m  # First  moment estimate.\n",
    "        v = (1 - b2) * (g**2) + b2 * v  # Second moment estimate.\n",
    "        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat = v / (1 - b2**(i + 1))\n",
    "        x = x - step_size*mhat/(np.sqrt(vhat) + eps)\n",
    "    return x,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argumentos óptimos:\n",
      "W = \t[-13.72208287  -7.949832    -5.95907525]\n",
      "w = \t[13.47579203  1.26226343  0.06519222]\n",
      "\t[ -8.15510649 -10.7536731    4.54539682]\n",
      "w_0 = \t[0.99780799 2.23119496]\n",
      "\n",
      "error: 9.753594713656057e-06\n",
      "Tiempo de ejecucion: 0.33398599499923876 segundos\n",
      "Iteraciones: 52\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.uniform(low=MIN_REAL, high=MAX_REAL, size=NUMBER_OF_PARAMETERS)\n",
    "gradient = nd.Gradient(error)\n",
    "\n",
    "def grad(x,i):\n",
    "    return gradient(x, DATASET_INPUT, DATASET_OUTPUT)\n",
    "\n",
    "\n",
    "def adam_optimization(x0):\n",
    "    start_time = time.perf_counter()\n",
    "    optimize_result_Adam,steps= adam(error,grad,x0,step_size= 0.8)\n",
    "    error_Adam = error(optimize_result_Adam, DATASET_INPUT, DATASET_OUTPUT)\n",
    "    end_time = time.perf_counter()\n",
    "    return optimize_result_Adam,end_time-start_time,error_Adam,steps\n",
    "\n",
    "\n",
    "optimize_result_Adam,time_Adam,error_Adam,steps = adam_optimization(x0)\n",
    "\n",
    "print_output(optimize_result_Adam, time_Adam, error_Adam,steps)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d39c3dda0d45297420c0d8fc1cf706a4a2909234f3473c84dddd2ffa0a641241"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('non-linear-optimization-methods-gr2b39L-')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
